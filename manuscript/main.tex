\documentclass[12pt]{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{amssymb}
\usepackage{bbm}

\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}
\usepackage{url}

%%% For figures
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

%%% For tables
\usepackage[table]{xcolor}

%specifying table and figure packages
\usepackage{caption}

\usepackage[T1]{fontenc}
\usepackage[all]{xy}
\usepackage[inline]{enumitem}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\newcommand{\mattia}[1]{\textcolor{cyan}{#1}}
\newcommand{\pietro}[1]{\textcolor{teal}{#1}}
\newcommand{\Ban}{{\mathbf{Ban}}}
\newcommand{\Top}{{\mathbf{Top}}}
\newcommand{\pt}{{\textnormal{pt}}}
\newcommand{\Hom}{{\textnormal{Hom}}}
\newcommand{\End}{{\textnormal{End}}}
\newcommand{\Fun}{{\textnormal{Fun}}}
\newcommand{\Aut}{{\textnormal{Aut}}}
\newcommand{\Obj}{{\textnormal{Obj}}}
\newcommand{\id}{{\textnormal{id}}}
\newcommand{\Morph}{{\textnormal{Morph}}}
\newcommand{\Set}{{\mathbf{Set}}}
\newcommand{\Cat}{{\mathbf{C}}}
\newcommand{\JCat}{{\mathcal{J}}}
\newcommand{\LCat}{{\mathbf{L}}}
\newcommand{\QCat}{{\mathbf{Q}}}
\newcommand{\range}[2]{{\{{#1}, \dots,{#2}\}}}
\newcommand{\anon}{{\,\mbox{-}\,}}

\crefname{diagram}{diag.}{diags.}
\Crefname{diagram}{Diagram}{Diagrams}
\creflabelformat{diagram}{#2(#1)#3}

\title{Weight sharing via comonads}
\author{
    Pietro Vertechi \and Mattia G. Bergomi
}
\date{}
\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Architecture}

\begin{definition}\label{def:architecture}
    An {\em architecture} is a closed monoidal category $(\Cat, \otimes)$, together with a monoidal comonad $T\colon \Cat \rightarrow \Cat$.
\end{definition}

\pietro{Questo teorema e' noto ma credo valga la pena menzionarlo.}

\begin{theorem}\label{thm:categorical_architecture}
    Let $\QCat$ be a locally Cartesian closed category. Then, there is a well defined notion of category {\em internal} to $\QCat$~\cite{mac2013categories}.
    Let $\JCat = s, t \colon C_1 \rightarrow C_0$ be such an internal category. $\JCat$ induces an architecture
    \begin{equation*}
        s_!t^*\colon \QCat/C_0 \rightarrow \QCat/C_0.
    \end{equation*}
\end{theorem}

\begin{proof}
    The map $t_*s^*\colon \QCat/C_0 \rightarrow \QCat/C_0$ is a monad~\cite[Thm.~V.8.2]{Mac_Lane_1994}. As $\QCat$ is locally Cartesian closed, $s^*$ has a right adjoint $s_!$, hence we can consider $s_!t^*$. As $T = s_!t^*$ is the right adjoint of the monad $t_*s^*$, it is a comonad. Being a right adjoint, $T$ preserves finite products.
\end{proof}

\pietro{Discutere ultima comonade di \cite{uustalu_comonadic_2008} come caso particolare.}

Let $T$ be an architecture on a closed monoidal category $(\Cat, \otimes)$. Let $\Cat^T$ be Eilenberg-Moore category for the comonad $T$. It is a known fact~\cite{Moerdijk_2002,pastro2009closed} that the monoidal structure on $\Cat$ lifts to a monoidal structure on $\Cat^T$ in such a way that the forgetful functor $U\colon \Cat^T \rightarrow \Cat$ is strict monoidal.

\begin{theorem}\label{thm:cofree_exponentiating}
    Let $T\colon \Cat \rightarrow \Cat$ be an architecture. Then cofree coalgebras in $\Cat^T$ are exponentiating. In particular, let $F\colon \Cat \rightarrow \Cat^T$ denote the cofree functor. Then, there exists a natural isomorphism
    \begin{equation*}
        \Hom(A\otimes B, FX) \simeq \Hom(A, F[UB, X]).
    \end{equation*}
\end{theorem}

\begin{proof}
    By a straightforward computation:
    \begin{align*}
        \Hom_{\Cat^T}(A\otimes B, FX)
        &\simeq \Hom_{\Cat}(U(A \otimes B), X)\\
        &\simeq \Hom_{\Cat}(UA \otimes UB, X)\\
        &\simeq \Hom_{\Cat}(UA, [UB, X])\\
        &\simeq \Hom_{\Cat^T}(A, F[UB, X]).
    \end{align*}
\end{proof}

\section{Equivariance}

One of the key reasons for the success of neural networks is the notion of {\em weight sharing}. In its simplest form, one can consider, among maps $[M, X] \rightarrow [M, X]$, those that originate from maps $[M, X] \rightarrow X$. Indeed, given
\begin{equation*}
    {\cal F} \colon [M, X] \rightarrow X,
\end{equation*}
one can consider the map
\begin{align*}
    \widehat{\cal F} \colon [M, X] &\rightarrow [M, X] \\
    \left(\widehat{\cal F}\phi\right)(m) &= {\cal F}(a \mapsto \phi(ma)).
\end{align*}

This simple notion of weight sharing does not seem sufficient to cover complex neural networks, made up of several nodes, each with their own symmetries, which interact with each other in non-trivial ways.

We aim to show that the simple notion of weight sharing defined above is sufficient to recover, among other examples, classical recurrent and convolutional neural networks. However, to do so, we will need to generalize not the weight sharing strategy, but rather the underlying category on which such strategy takes place.

Here, we consider how to define a notion of equivariance given an architecture $T\colon\Cat \rightarrow \Cat$.

\begin{theorem}\label{thm:weight_sharing}
    Let $M \in \Cat^T$ be a monoid. Then $M$ induces a comonad:
    \begin{equation*}
        X \mapsto T[UM, X].
    \end{equation*}
\end{theorem}

\begin{proof}
    \pietro{Use $F[UM, X]$ and Kleisli category. Attenzione: Kleisli non ha per forza un prodotto tensore.} Monoidal functor
    \begin{equation*}
        \left(\Cat^T\right)^{op} \rightarrow \Fun(\Cat_T, \Cat_T),
    \end{equation*}
    hence monoid goes to comonad. That, and free forget from Kleisli to $\Cat$.
\end{proof}

\section{Locality}

\pietro{Fai la localita' in termini di sottoggetti di M o qualcosa di simile. Funziona?}

\section{Comonadic machines}

Show how it's possible to obtain a machine from an architecture.

\section{Recurrent and convolutional neural networks}

Monoids can be considered as categories with one object. Let us consider the category corresponding to $\mathbb N$. Let $M$ be the set containing only the generator morphism. Then the above construction recovers recurrent neural networks. Analogously, considering $\mathbb N \times \mathbb N$ we obtain convolutional neural networks, where $M$ denotes the shape of the filter.
\pietro{TODO: mention 2 objects for conv, and strides!}
\pietro{Maybe also do functors, to generalize GENEO? Somehow discuss continuous version?}

\section{Novel architectures}

Here, we explore the scenario where the category $\Cat$ has more than one object.
\pietro{TODO: figure out a good application.}
\pietro{Mention partial symmetries because of non-composability.}

\bibliographystyle{abbrv}
\bibliography{References}

\end{document}
